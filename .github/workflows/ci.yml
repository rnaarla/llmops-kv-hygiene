name: kv-hygiene-ci

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 6 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  packages: write
  security-events: write

jobs:
  generate-matrix:
    name: Generate test matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.mk.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
      - id: mk
        name: Build matrix JSON
        run: |
          python - <<'PY'
          import json, os, sys
          path = 'ci_cd/test_matrix.json'
          with open(path,'r',encoding='utf-8') as f: cfg=json.load(f)
          # Normalize keys -> GitHub matrix format
          matrix = {
            'os': cfg.get('platform', ['ubuntu-latest']),
            'python-version': cfg.get('python', ['3.11'])
          }
          print('Matrix:', matrix)
          # Write to GITHUB_OUTPUT
          print(f"matrix={json.dumps(matrix)}", file=open(os.environ['GITHUB_OUTPUT'],'a'))
          PY

  lint:
    name: Lint (ruff/black/mypy)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install dev dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Type ignore policy audit
        run: python tools/audit_type_ignores.py
      - name: Ruff
        run: ruff check . --output-format=github
      - name: Black check
        run: black --check .
      - name: Mypy (tools+tests via config) with cache
        env:
          MYPY_CACHE_DIR: .mypy_cache
        run: mypy --config-file mypy.ini tools tests
      - name: Upload mypy cache
        if: always()
        uses: actions/upload-artifact@v4
        with:
            name: mypy-cache
            path: .mypy_cache
      - name: Tool versions summary
        if: always()
        run: |
          echo "Python $(python -V)" || true
          ruff --version || true
          black --version || true
          mypy --version || true

  test:
    name: Unit tests and hygiene gates
    needs: [lint, generate-matrix]
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run tests with coverage (XML + JUnit + raw)
        run: |
          mkdir -p reports
          # Coverage gate raised from 87 -> 90 after expanded test suite & hygiene improvements
          pytest --maxfail=1 --disable-warnings -q \
            --cov=tools --cov-report=term-missing \
            --cov-report=xml:reports/coverage.xml \
            --cov-report=json:reports/coverage.json \
            --junitxml=reports/junit.xml \
            --cov-fail-under=90
      - name: Upload coverage XML
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.os }}-${{ matrix.python-version }}
          path: reports/coverage.xml
      - name: Upload raw coverage JSON
        uses: actions/upload-artifact@v4
        with:
          name: coverage-raw-${{ matrix.os }}-${{ matrix.python-version }}
          path: reports/coverage.json

      - name: Generate metrics JSON and verify HMAC chain
        env:
          FORENSIC_HMAC_SECRET: ci-secret
        run: |
          python - <<'PY'
          import json
          from tools.cache_tracer import CacheTracer, ForensicLogger
          t = CacheTracer()
          h = t.allocate(tenant_id="ci", request_id="ci-1", model_id="m", shape=(128,), dtype="float32", device="cpu", framework="numpy")
          t.mark_in_use(h)
          cov = t.sanitize(h, async_=False, verify=True)
          try:
              t.free(h)
          except Exception:
              pass
          t.export_metrics("forensics/coverage.json")
          res = ForensicLogger.verify_chain("forensics/kv_cache.log")
          print("Coverage:", cov)
          print("Metrics:", open("forensics/coverage.json").read())
          print("Forensic chain:", json.dumps(res))
          PY

      - name: Verify rotated forensic chain (all files)
        run: |
          python - <<'PY'
          import sys, json
          from tools.cache_tracer import ForensicLogger
          res = ForensicLogger.verify_all('forensics/kv_cache.log')
          print(json.dumps(res))
          sys.exit(0 if res.get('ok') else 2)
          PY

      - name: Run eviction checker (enforce thresholds)
        run: |
          python tools/eviction_checker.py forensics/coverage.json --coverage-min 99.9 --unsanitized-max 0 --quarantine-max 0 --out forensics/verdict.json
          cat forensics/verdict.json

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          # Include python version to ensure uniqueness across matrix jobs
          name: reports-and-forensics-${{ matrix.os }}-${{ matrix.python-version }}
          path: |
            reports/**
            forensics/**

  coverage-aggregate:
    name: Aggregate coverage
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v4
      - name: Download coverage artifacts
        uses: actions/download-artifact@v4
        with:
          path: coverage_artifacts
          pattern: coverage-raw-*
          merge-multiple: true
      - name: Combine coverage (lines only) & report
        run: |
          python - <<'PY'
          import json, pathlib, statistics
          files = list(pathlib.Path('coverage_artifacts').rglob('coverage.json'))
          total_covered=0
          total_statements=0
          for fp in files:
              data=json.loads(fp.read_text())
              # Coverage json produced by coverage.py has top-level keys; we locate totals
              totals=data.get('totals') or {}
              total_covered += totals.get('covered_lines',0)
              total_statements += totals.get('num_statements',0)
          pct = (total_covered/total_statements*100) if total_statements else 0.0
          print(f'Aggregated coverage: {pct:.2f}% ({total_covered}/{total_statements})')
          # Enforce same threshold across aggregate (>=90)
          import sys
          sys.exit(0 if pct >= 90.0 else 2)
          PY

  docker-images:
    name: Build and push Docker images (CPU + CUDA)
    runs-on: ubuntu-latest
    needs: [coverage-aggregate]
    if: github.event_name != 'pull_request'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Prepare tags
        id: prep
        run: |
          REPO_LOWER=$(echo "${{ github.repository }}" | tr '[:upper:]' '[:lower:]')
          echo "repo=ghcr.io/${REPO_LOWER}" >> $GITHUB_OUTPUT
          SHA_TAG=${GITHUB_SHA::12}
          echo "sha=${SHA_TAG}" >> $GITHUB_OUTPUT

      - name: Build and push CPU image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile.cpu
          platforms: linux/amd64
          push: true
          tags: |
            ${{ steps.prep.outputs.repo }}:cpu-latest
            ${{ steps.prep.outputs.repo }}:cpu-${{ steps.prep.outputs.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push CUDA image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile.cuda
          platforms: linux/amd64
          push: true
          tags: |
            ${{ steps.prep.outputs.repo }}:cuda-latest
            ${{ steps.prep.outputs.repo }}:cuda-${{ steps.prep.outputs.sha }}
          build-args: |
            TORCH_INDEX_URL=https://download.pytorch.org/whl/cu121
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Trivy scan (CPU)
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: ${{ steps.prep.outputs.repo }}:cpu-${{ steps.prep.outputs.sha }}
          format: sarif
          output: trivy-cpu.sarif
          severity: CRITICAL,HIGH
          exit-code: '1'

      - name: Trivy scan (CUDA)
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: ${{ steps.prep.outputs.repo }}:cuda-${{ steps.prep.outputs.sha }}
          format: sarif
          output: trivy-cuda.sarif
          severity: CRITICAL,HIGH
          exit-code: '1'

      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: trivy-cpu.sarif
        continue-on-error: true

      - name: Upload Trivy results (CUDA)
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: trivy-cuda.sarif
        continue-on-error: true

      - name: Generate SBOM (Syft CPU)
        uses: anchore/sbom-action@v0
        with:
          image: ${{ steps.prep.outputs.repo }}:cpu-${{ steps.prep.outputs.sha }}
          output-file: sbom-cpu-${{ steps.prep.outputs.sha }}.spdx.json

      - name: Generate SBOM (Syft CUDA)
        uses: anchore/sbom-action@v0
        with:
          image: ${{ steps.prep.outputs.repo }}:cuda-${{ steps.prep.outputs.sha }}
          output-file: sbom-cuda-${{ steps.prep.outputs.sha }}.spdx.json

      - name: Upload SBOM artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sbom
          path: |
            sbom-*.spdx.json
